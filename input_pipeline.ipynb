{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11735986,"sourceType":"datasetVersion","datasetId":7367587}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:27:28.156523Z","iopub.execute_input":"2025-05-08T16:27:28.156783Z","iopub.status.idle":"2025-05-08T16:27:30.667103Z","shell.execute_reply.started":"2025-05-08T16:27:28.156762Z","shell.execute_reply":"2025-05-08T16:27:30.665839Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Description\nThis pipeline extracts and processes audio, visual and text features from videos using deep learning models.","metadata":{}},{"cell_type":"markdown","source":"# Importing required libraries ","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom typing import Dict, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torchaudio\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torchvision.models import resnet18\nfrom PIL import Image\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:27:42.063919Z","iopub.execute_input":"2025-05-08T16:27:42.064272Z","iopub.status.idle":"2025-05-08T16:27:53.865339Z","shell.execute_reply.started":"2025-05-08T16:27:42.064247Z","shell.execute_reply":"2025-05-08T16:27:53.864391Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import (\n    WhisperProcessor, WhisperForConditionalGeneration,\n    BertTokenizer, BertModel\n)\nimport librosa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:28:06.018506Z","iopub.execute_input":"2025-05-08T16:28:06.020684Z","iopub.status.idle":"2025-05-08T16:28:32.895317Z","shell.execute_reply.started":"2025-05-08T16:28:06.020634Z","shell.execute_reply":"2025-05-08T16:28:32.894195Z"}},"outputs":[{"name":"stderr","text":"2025-05-08 16:28:16.278819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746721696.541422      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746721696.618885      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Setting up the parameters","metadata":{}},{"cell_type":"code","source":"VIDEO_PATH = \"/kaggle/input/abcdefg/SampleVideo_1280x720_1mb.mp4\" #provide video path here\nSAMPLE_RATE = 16000  # Audio sample rate (Hz)\nN_MFCC = 5           # Number of MFCC features\nVISION_FEATURES = 20 # Visual feature dimension\nMAX_TIMESTEPS = 50   # Max time steps for alignment\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:33:51.608818Z","iopub.execute_input":"2025-05-08T16:33:51.609223Z","iopub.status.idle":"2025-05-08T16:33:51.615452Z","shell.execute_reply.started":"2025-05-08T16:33:51.609198Z","shell.execute_reply":"2025-05-08T16:33:51.614241Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# FFmpeg Command Execution\nThis helper function runs an FFmpeg command in the shell and returns `True` if the command succeeds, or `False` if it fails. It also prints useful error messages if something goes wrong.\n\n**Function Purpose:**  To safely execute FFmpeg commands and handle errors ","metadata":{}},{"cell_type":"code","source":"def run_ffmpeg_command(cmd: str) -> bool:\n    \"\"\"Execute an ffmpeg command and return True if successful.\"\"\"\n    try:\n        subprocess.run(\n            cmd.split(),\n            check=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"FFmpeg failed: {e.stderr.decode()}\")\n        return False\n    except Exception as e:\n        print(f\"Error running FFmpeg: {str(e)}\")\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:28:56.616524Z","iopub.execute_input":"2025-05-08T16:28:56.616963Z","iopub.status.idle":"2025-05-08T16:28:56.623536Z","shell.execute_reply.started":"2025-05-08T16:28:56.616937Z","shell.execute_reply":"2025-05-08T16:28:56.622256Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Feature Alignment","metadata":{}},{"cell_type":"code","source":"def align_features(features: np.ndarray, max_len: int = MAX_TIMESTEPS) -> np.ndarray:\n    \"\"\"Align features to a fixed length by padding or truncating.\"\"\"\n    if features is None or features.size == 0:\n        # Return zeros with the expected shape if features are empty\n        return np.zeros((max_len, features.shape[1])) if len(features.shape) > 1 else np.zeros(max_len)\n    if len(features) > max_len:\n        return features[:max_len]\n    elif len(features) < max_len:\n        pad_shape = (max_len - len(features),) + features.shape[1:]\n        return np.concatenate([features, np.zeros(pad_shape)], axis=0)\n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:29:07.647723Z","iopub.execute_input":"2025-05-08T16:29:07.648125Z","iopub.status.idle":"2025-05-08T16:29:07.655650Z","shell.execute_reply.started":"2025-05-08T16:29:07.648099Z","shell.execute_reply":"2025-05-08T16:29:07.654299Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Feature Normalization\nThis function normalizes each feature tensor in the input dictionary to the range `[0, 1]`. This ensures that all modalities are on a similar scale to help model train more efficiently.","metadata":{}},{"cell_type":"code","source":"def normalize_features(features: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    \"\"\"Normalize each feature tensor to [0, 1] range.\"\"\"\n    for modality in features:\n        feat = features[modality]\n        if feat.nelement() == 0:\n            continue\n        min_val = feat.min()\n        max_val = feat.max()\n        features[modality] = (feat - min_val) / (max_val - min_val + 1e-8)\n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:29:16.550278Z","iopub.execute_input":"2025-05-08T16:29:16.550674Z","iopub.status.idle":"2025-05-08T16:29:16.556706Z","shell.execute_reply.started":"2025-05-08T16:29:16.550630Z","shell.execute_reply":"2025-05-08T16:29:16.555695Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Audio Processing\nExtracts MFCC and their delta features from audio tracks in video files","metadata":{}},{"cell_type":"code","source":"class AudioProcessor:\n    \"\"\"Extracts MFCC features and deltas from audio.\"\"\"\n    def __init__(self):\n        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=SAMPLE_RATE,\n            n_mels=N_MFCC,\n            n_fft=400,\n            hop_length=160\n        ).to(DEVICE)\n\n    def extract_features(self, video_path: str) -> Optional[np.ndarray]:\n        \"\"\"Extract and process audio features from video file.\"\"\"\n        audio_temp = \"temp_audio.wav\"\n        cmd = f\"ffmpeg -y -i {video_path} -vn -acodec pcm_s16le -ar {SAMPLE_RATE} {audio_temp}\"\n        if not run_ffmpeg_command(cmd):\n            return None\n\n        if not os.path.exists(audio_temp):\n            print(f\"Audio file {audio_temp} not created!\")\n            return None\n\n        try:\n            waveform, _ = torchaudio.load(audio_temp)\n            if waveform.nelement() == 0:\n                raise ValueError(\"Empty audio file\")\n            waveform = waveform.to(DEVICE)\n\n            # Extract MFCCs and deltas\n            mfcc = torchaudio.compliance.kaldi.mfcc(\n                waveform,\n                sample_frequency=SAMPLE_RATE,\n                num_ceps=N_MFCC\n            )\n            deltas = torchaudio.functional.compute_deltas(mfcc)\n            features = torch.cat([mfcc, deltas], dim=-1)[..., :N_MFCC]\n            return features.squeeze(0).cpu().numpy()\n\n        except Exception as e:\n            print(f\"Audio processing failed: {str(e)}\")\n            return None\n        finally:\n            if os.path.exists(audio_temp):\n                os.remove(audio_temp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:29:35.389136Z","iopub.execute_input":"2025-05-08T16:29:35.389517Z","iopub.status.idle":"2025-05-08T16:29:35.398658Z","shell.execute_reply.started":"2025-05-08T16:29:35.389489Z","shell.execute_reply":"2025-05-08T16:29:35.397560Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Vision Feature Extraction\nExtracts visual features from video frames using a pretrained ResNet18 model.","metadata":{}},{"cell_type":"code","source":"class VisionProcessor:\n    \"\"\"Extracts visual features from video frames using ResNet18.\"\"\"\n    def __init__(self):\n        self.model = resnet18(weights=\"DEFAULT\")  # Use latest weights\n        self.model.fc = nn.Linear(512, VISION_FEATURES)\n        self.model = self.model.to(DEVICE).eval()\n\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.205]\n            )\n        ])\n\n    def extract_features(self, video_path: str) -> Optional[np.ndarray]:\n        \"\"\"Extract visual features from video frames.\"\"\"\n        if not os.path.exists(video_path):\n            print(f\"Video file {video_path} not found!\")\n            return None\n\n        try:\n            cap = cv2.VideoCapture(video_path)\n            if not cap.isOpened():\n                raise ValueError(\"Could not open video file\")\n\n            frames = []\n            fps = cap.get(cv2.CAP_PROP_FPS)\n            frame_interval = max(1, int(fps))  # At least 1 frame per second\n            frame_count = 0\n\n            while cap.isOpened():\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                if frame_count % frame_interval == 0:\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                    frames.append(Image.fromarray(frame))\n                frame_count += 1\n            cap.release()\n\n            if not frames:\n                raise ValueError(\"No frames extracted\")\n\n            features = []\n            for frame in frames:\n                tensor = self.transform(frame).unsqueeze(0).to(DEVICE)\n                with torch.no_grad():\n                    feat = self.model(tensor)\n                features.append(feat.squeeze().cpu().numpy())\n            return np.array(features)\n\n        except Exception as e:\n            print(f\"Vision processing failed: {str(e)}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:30:28.754386Z","iopub.execute_input":"2025-05-08T16:30:28.754766Z","iopub.status.idle":"2025-05-08T16:30:28.767297Z","shell.execute_reply.started":"2025-05-08T16:30:28.754740Z","shell.execute_reply":"2025-05-08T16:30:28.765993Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Text Feature Extraction\nHandles **speech-to-text transcription** (using Whisper ASR) and **text embedding generation** (using BERT).","metadata":{}},{"cell_type":"code","source":"class TextProcessor:\n    \"\"\"Transcribes audio to text (ASR) and extracts BERT embeddings.\"\"\"\n    def __init__(self):\n        # ASR (Whisper)\n        self.asr_processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n        self.asr_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(DEVICE)\n        # BERT Embeddings\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.bert = BertModel.from_pretrained('bert-base-uncased').to(DEVICE)\n\n    def extract_features(self, video_path: str) -> Optional[np.ndarray]:\n        \"\"\"Transcribe audio and extract BERT embeddings.\"\"\"\n        audio_temp = \"temp_audio_asr.wav\"\n        cmd = f\"ffmpeg -y -i {video_path} -vn -acodec pcm_s16le -ar {SAMPLE_RATE} {audio_temp}\"\n        if not run_ffmpeg_command(cmd):\n            return None\n\n        if not os.path.exists(audio_temp):\n            print(f\"ASR audio file {audio_temp} not created!\")\n            return None\n\n        try:\n            audio, _ = librosa.load(audio_temp, sr=16000)\n            if len(audio) == 0:\n                raise ValueError(\"Empty audio for ASR\")\n\n            inputs = self.asr_processor(\n                audio,\n                sampling_rate=SAMPLE_RATE,\n                return_tensors=\"pt\"\n            ).input_features.to(DEVICE)\n\n            with torch.no_grad():\n                predicted_ids = self.asr_model.generate(inputs)\n            text = self.asr_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n\n            if not text.strip():\n                raise ValueError(\"No text transcribed\")\n\n            inputs = self.tokenizer(\n                text,\n                return_tensors=\"pt\",\n                truncation=True,\n                padding='max_length',\n                max_length=MAX_TIMESTEPS\n            ).to(DEVICE)\n\n            with torch.no_grad():\n                outputs = self.bert(**inputs)\n            return outputs.last_hidden_state.squeeze(0).cpu().numpy()\n\n        except Exception as e:\n            print(f\"Text processing failed: {str(e)}\")\n            return None\n        finally:\n            if os.path.exists(audio_temp):\n                os.remove(audio_temp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:30:50.584479Z","iopub.execute_input":"2025-05-08T16:30:50.584840Z","iopub.status.idle":"2025-05-08T16:30:50.596883Z","shell.execute_reply.started":"2025-05-08T16:30:50.584817Z","shell.execute_reply":"2025-05-08T16:30:50.595382Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Multimodal Feature Pipeline\nIntegrates audio, visual, and text processing to create aligned, normalized multimodal features from video inputs.","metadata":{}},{"cell_type":"code","source":"class MultimodalPipeline:\n    \"\"\"Processes video to extract aligned and normalized audio, visual, and text features.\"\"\"\n    def __init__(self):\n        self.audio_processor = AudioProcessor()\n        self.vision_processor = VisionProcessor()\n        self.text_processor = TextProcessor()\n\n    def process_video(self, video_path: str) -> Optional[Dict[str, torch.Tensor]]:\n        \"\"\"Extract and align features from audio, vision, and text modalities.\"\"\"\n        if not os.path.exists(video_path):\n            print(f\"Video file {video_path} not found!\")\n            return None\n\n        try:\n            audio = self.audio_processor.extract_features(video_path)\n            vision = self.vision_processor.extract_features(video_path)\n            text = self.text_processor.extract_features(video_path)\n\n            if audio is None or vision is None or text is None:\n                print(\"Feature extraction failed for one or more modalities\")\n                return None\n\n            audio = align_features(audio)\n            vision = align_features(vision)\n            text = align_features(text)\n\n            features = {\n                \"audio\": torch.tensor(audio).unsqueeze(0).float(),\n                \"vision\": torch.tensor(vision).unsqueeze(0).float(),\n                \"text\": torch.tensor(text).unsqueeze(0).float()\n            }\n\n            features = normalize_features(features)\n            return features\n\n        except Exception as e:\n            print(f\"Pipeline failed: {str(e)}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:31:13.075378Z","iopub.execute_input":"2025-05-08T16:31:13.075811Z","iopub.status.idle":"2025-05-08T16:31:13.084572Z","shell.execute_reply.started":"2025-05-08T16:31:13.075716Z","shell.execute_reply":"2025-05-08T16:31:13.083275Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Main Execution Block\nRuns the entire multimodal feature extraction pipeline on the input video. Note that it will work only if internet access is properly configured(for downloading pretrained models).","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    pipeline = MultimodalPipeline()\n    features = pipeline.process_video(VIDEO_PATH)\n\n    if features is not None:\n        print(f\"Audio features shape: {features['audio'].shape}\")  # (1, 50, 5)\n        print(f\"Vision features shape: {features['vision'].shape}\")  # (1, 50, 20)\n        print(f\"Text features shape: {features['text'].shape}\")  # (1, 50, 768)\n    else:\n        print(\"Pipeline failed to process video\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:37:20.208536Z","iopub.execute_input":"2025-05-08T16:37:20.208999Z","iopub.status.idle":"2025-05-08T16:38:20.584824Z","shell.execute_reply.started":"2025-05-08T16:37:20.208962Z","shell.execute_reply":"2025-05-08T16:38:20.583699Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 67.3MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e1a4a4dd6904c38870e92d6a6e9ed18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a29247fbc486467ab7849760f878c64b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee5b80d5d1d5493490f0b28ed9c3152e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a735d969ea9041ebaaddb32dce1c972c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f540054c2e0840a38eab81295fc76711"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc97706688b7477d87ca274cab559fef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f35723ba9e4f9188fad1f5905e2782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47f8e8add2ab4bca8ca4db28b62ba416"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c32a91257f054a0f9f9d2a656c229650"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64113ada54a64a44923668fe402b3717"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"907f68303c0d4768b41c64bb60f3b286"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f97060521e57472aae9d263e36f19b1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c01198ca50748b5ae52239dd4773a47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f22be0080e7b4be2a19f6d75438c308b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31f4e7d1ce884bc4ba9fe6b2d5f31120"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a732edfac1004951bffad838e4f40b15"}},"metadata":{}},{"name":"stderr","text":"Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Audio features shape: torch.Size([1, 50, 5])\nVision features shape: torch.Size([1, 50, 20])\nText features shape: torch.Size([1, 50, 768])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}